{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff76b937",
   "metadata": {},
   "source": [
    "# DS 542 Document Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161c8ff",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdfe7f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbf1c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: /home/cethan/GitHub/542-LegalContract-AI/Datasets/processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cethan/GitHub/542-LegalContract-AI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from find_dataset import locate\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATA = locate(\"Datasets\") / \"processed\"\n",
    "\n",
    "print(f\"Datasets: {DATA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da52ab9",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e87747",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA / \"contractnli.json\") as f:\n",
    "    cnli = json.load(f)\n",
    "\n",
    "with open(DATA / \"cuad.json\") as f:\n",
    "    cuad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7db506",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuad_text = [i[\"text\"] for i in cuad]\n",
    "cnli_text = [i[\"text\"] for i in cnli]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bc51c",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a8602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    \"\"\"Plot the top words for each topic in the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        The topic model (NMF, LDA, etc.) that contains the components_ attribute\n",
    "    feature_names : array-like\n",
    "        The names of the features (words) corresponding to each component\n",
    "    n_top_words : int\n",
    "        Number of top words to display for each topic\n",
    "    title : str\n",
    "        Title for the plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays a matplotlib figure with subplots for each topic\n",
    "\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(12, 5), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx + 1}\", fontdict={\"fontsize\": 14})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "        for i in [\"top\", \"right\", \"left\"]:\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35b75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, vectorizer, lda_model):\n",
    "    \"\"\"Classify a text using the trained LDA model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to classify\n",
    "    vectorizer : CountVectorizer\n",
    "        The fitted vectorizer\n",
    "    lda_model : LatentDirichletAllocation\n",
    "        The fitted LDA model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    topic_probs : numpy array\n",
    "        Probability distribution over topics\n",
    "    dominant_topic : int\n",
    "        The index of the dominant topic\n",
    "\n",
    "    \"\"\"\n",
    "    # Transform the text using the vectorizer\n",
    "    text_tf = vectorizer.transform([text])\n",
    "\n",
    "    # Get the topic distribution for the text\n",
    "    topic_probs = lda_model.transform(text_tf)[0]\n",
    "\n",
    "    # Get the dominant topic\n",
    "    dominant_topic = topic_probs.argmax()\n",
    "\n",
    "    return topic_probs, dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85019a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_process(\n",
    "    dataset: list,\n",
    "    n_features: int = 1000,\n",
    "    n_components: int = 5,\n",
    "    n_top_words: int = 10,\n",
    "    batch_size: int = 128,\n",
    "    init: str = \"nndsvda\",\n",
    "):\n",
    "    \"\"\"Create and process the dataset for topic modeling.\"\"\"\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=1, max_features=n_features, stop_words=\"english\")\n",
    "    vectorizer.fit(dataset)\n",
    "    tf = vectorizer.transform(dataset)\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_components,\n",
    "        max_iter=10,\n",
    "        learning_method=\"online\",\n",
    "        learning_offset=50.0,\n",
    "        random_state=542,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    lda.fit(tf)\n",
    "\n",
    "    return lda, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c164d",
   "metadata": {},
   "source": [
    "## Vectorizer and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0977364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = 5\n",
    "n_top_words = 10\n",
    "batch_size = 128\n",
    "init = \"nndsvda\"\n",
    "\n",
    "doc_lda, doc_vec = create_process(\n",
    "    cuad_text + cnli_text,\n",
    "    n_features=n_features,\n",
    "    n_components=n_components,\n",
    "    n_top_words=n_top_words,\n",
    "    batch_size=batch_size,\n",
    "    init=init,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b8eaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1117/1117 [26:53<00:00,  1.44s/it] \n"
     ]
    }
   ],
   "source": [
    "full_process = []\n",
    "\n",
    "for doc in tqdm(cuad + cnli):\n",
    "    doc_topic_probs, doc_dominant_topic = classify_text(doc[\"text\"], doc_vec, doc_lda)\n",
    "\n",
    "    temp = {\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"file_name\": doc[\"file_name\"],\n",
    "        \"full_text\": doc[\"text\"],\n",
    "        \"full_text_topic_num\": n_components,\n",
    "        \"full_text_topic_label\": int(doc_dominant_topic),\n",
    "        \"lines\": [],\n",
    "        \"lines_topic_num\": n_components,\n",
    "        \"line_topic_label\": [],\n",
    "    }\n",
    "\n",
    "    doc_lines = [i for i in doc[\"text\"].split(\"\\n\") if i.strip()]\n",
    "\n",
    "    if len(doc_lines) == 1:\n",
    "        temp[\"lines\"].append(doc_lines[0])\n",
    "        temp[\"line_topic_label\"].append(0)\n",
    "        full_process.append(temp)\n",
    "        continue\n",
    "\n",
    "    temp_lda, temp_vec = create_process(\n",
    "        doc_lines,\n",
    "        n_features=n_features,\n",
    "        n_components=n_components,\n",
    "        n_top_words=n_top_words,\n",
    "        batch_size=batch_size,\n",
    "        init=init,\n",
    "    )\n",
    "\n",
    "    for line in doc_lines:\n",
    "        line_topic_probs, line_dominant_topic = classify_text(line, temp_vec, temp_lda)\n",
    "\n",
    "        temp[\"lines\"].append(line)\n",
    "        temp[\"line_topic_label\"].append(int(line_dominant_topic))\n",
    "\n",
    "    full_process.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "889c8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA / \"complete.json\", \"w\") as f:\n",
    "    json.dump(full_process, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183edc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
