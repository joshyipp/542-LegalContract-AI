{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clause Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\DS340\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = AutoModelForPreTraining.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure no truncation of column content\n",
    "pd.set_option('display.max_rows', None)     # Display all rows\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Datasets/master_clauses.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df['Filename'] = df['Filename'].str.replace('.pdf', '.txt', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Sentence for Terms and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh_\\AppData\\Local\\Temp\\ipykernel_21940\\2216313821.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_to_flatten[column] = df_to_flatten[column].apply(ensure_list)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This Agreement is accepted by Company in the State of Nevada and shall be governed by and construed in accordance with the laws thereof, which laws shall prevail in the event of any conflict.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Company shall not specify the business practices of MA, nor regulate the manner in which MA shall operate its business, provided that MA (a) conducts business in a manner that reflects favorably at all times on the Technology sold and the good name, goodwill and reputation of Company and its affiliates&lt;omitted&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This Agreement is subject to all laws, regulations, license conditions and decisions of the Canadian Radio-television and Telecommunications Commission (\"CRTC\") municipal, provincial and federal governments or other authorities which are applicable to Rogers and/or Licensor, and which are now in force or hereafter adopted (\"Applicable Law\").</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Agreement shall be governed by laws of the Province of Ontario and the federal laws of Canada applicable therein.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All questions with respect to the construction of this Agreement, and the rights and liabilities of the Parties hereto, shall be governed by the laws of the State of Florida.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                          This Agreement is accepted by Company in the State of Nevada and shall be governed by and construed in accordance with the laws thereof, which laws shall prevail in the event of any conflict.   \n",
       "1                                 Company shall not specify the business practices of MA, nor regulate the manner in which MA shall operate its business, provided that MA (a) conducts business in a manner that reflects favorably at all times on the Technology sold and the good name, goodwill and reputation of Company and its affiliates<omitted>   \n",
       "2  This Agreement is subject to all laws, regulations, license conditions and decisions of the Canadian Radio-television and Telecommunications Commission (\"CRTC\") municipal, provincial and federal governments or other authorities which are applicable to Rogers and/or Licensor, and which are now in force or hereafter adopted (\"Applicable Law\").   \n",
       "3                                                                                                                                                                                                                                   This Agreement shall be governed by laws of the Province of Ontario and the federal laws of Canada applicable therein.   \n",
       "4                                                                                                                                                                           All questions with respect to the construction of this Agreement, and the rights and liabilities of the Parties hereto, shall be governed by the laws of the State of Florida.   \n",
       "\n",
       "   labels  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Step 1: Ensure all column values are lists\n",
    "def ensure_list(value):\n",
    "    if isinstance(value, list):\n",
    "        return value  # Already a list\n",
    "    elif isinstance(value, str) and value.startswith('[') and value.endswith(']'):\n",
    "        # If it's a string that looks like a list, try to parse it\n",
    "        try:\n",
    "            import ast\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return [value]  # If parsing fails, wrap it as a single-item list\n",
    "    elif pd.notnull(value):\n",
    "        return [value]  # If it's a scalar, wrap it as a single-item list\n",
    "    else:\n",
    "        return []  # Handle NaN or None values as empty lists\n",
    "\n",
    "# Combine text columns to form the input for tokenization\n",
    "columns_to_include = [\n",
    "    'Termination For Convenience',\n",
    "    'Post-Termination Services',\n",
    "    'Renewal Term',\n",
    "    'Notice Period To Terminate Renewal',\n",
    "    'Change Of Control',\n",
    "    'Liquidated Damages',\n",
    "    'Anti-Assignment'\n",
    "]\n",
    "\n",
    "# Load the ContractBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "df_to_flatten = df[columns_to_include]\n",
    "\n",
    "\n",
    "for column in df_to_flatten.columns:\n",
    "    df_to_flatten[column] = df_to_flatten[column].apply(ensure_list)\n",
    "\n",
    "# Step 2: Flatten the dataframe\n",
    "flattened_rows = []\n",
    "for index, row in df_to_flatten.iterrows():\n",
    "    for column in df_to_flatten.columns:\n",
    "        for clause in row[column]:\n",
    "            flattened_rows.append({'Category': column, 'text': clause})\n",
    "\n",
    "flattened_df = pd.DataFrame(flattened_rows)\n",
    "# Flatten the \"Clause\" column: Convert lists into plain strings\n",
    "flattened_df['text'] = flattened_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "# Remove rows with empty \"Clause\"\n",
    "flattened_df = flattened_df[flattened_df['text'].str.strip() != \"\"]\n",
    "# Create a new dataframe\n",
    "df_positives = pd.DataFrame(flattened_df['text'])\n",
    "\n",
    "df_positives['labels'] = 1\n",
    "\n",
    "# Display the flattened dataframe\n",
    "df_positives.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns to train against (the non positive labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'labels'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_columns = ['Parties','Agreement Date','Effective Date', 'Expiration Date', \"Filename\", \"Document Name\", \"Document Name-Answer\"]\n",
    "dynamic_exclude_columns = [col for col in df.columns if \"-Answer\" in col]\n",
    "all_columns_to_exclude = set(exclude_columns + dynamic_exclude_columns+columns_to_include)\n",
    "df_negatives= df.drop(columns=all_columns_to_exclude)\n",
    "\n",
    "\n",
    "for column in df_negatives.columns:\n",
    "    df_negatives[column] = df_negatives[column].apply(ensure_list)\n",
    "\n",
    "# Step 2: Flatten the dataframe\n",
    "flattened_rows = []\n",
    "for index, row in df_negatives.iterrows():\n",
    "    for column in df_negatives.columns:\n",
    "        for clause in row[column]:\n",
    "            flattened_rows.append({'Category': column, 'text': clause})\n",
    "\n",
    "flattened_df = pd.DataFrame(flattened_rows)\n",
    "# Flatten the \"Clause\" column: Convert lists into plain strings\n",
    "flattened_df['text'] = flattened_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "# Remove rows with empty \"Clause\"\n",
    "flattened_df = flattened_df[flattened_df['text'].str.strip() != \"\"]\n",
    "\n",
    "# Create a new dataframe\n",
    "df_negatives = pd.DataFrame(flattened_df['text'])\n",
    "\n",
    "df_negatives['labels'] = 0\n",
    "\n",
    "# Display the flattened dataframe\n",
    "df_negatives.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the datasets after labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8783, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the dataframes\n",
    "merged_df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "merged_df = merged_df[['text','labels']]\n",
    "\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "# Step 1: Randomly split the dataset into train and test\n",
    "random.seed(42)  # Set seed for reproducibility\n",
    "train_indices = random.sample(range(len(merged_df)), int(0.8 * len(merged_df)))  # 80% train\n",
    "test_indices = list(set(range(len(merged_df))) - set(train_indices))      # Remaining 20% test\n",
    "\n",
    "train_df = merged_df.iloc[train_indices].reset_index(drop=True)\n",
    "test_df = merged_df.iloc[test_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the event of any termination of the Project by University, (a) University agrees to complete Phase I and II of the Project, and (b) ArTara will continue to provide annual funding until the completion of Phase II.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Company agrees that no other Distributor will be appointed in any other state as a Distributor unless it is either the Company or Distributor, save and except for the state of Florida.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This Agreement shall automatically terminate in the event the Management Agreement is assigned or otherwise terminated.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You agree that, at our option, you will sell to us any or all your assets used to operate the Franchised Business (including equipment, fixtures, furnishings, Delivery Vehicles, supplies, and inventory) that we ask in writing to purchase. 16.2.1. The purchase price for such items will be equal to your depreciated cost (determined below) or fair market value, whichever is less. The cost will be determined based upon a five (5) year straight-line depreciation of original costs. For equipment that is five (5) or more years old, the parties agree that fair market value will be deemed to be ten percent (10%) of the equipment's original cost. The fair market value of tangible assets must be determined without reference to good will, going-concern value, or other intangible assets. Page 32 of 39\\n\\nSource: PF HOSPITALITY GROUP INC., 10-12G, 9/23/2015\\n\\n\\n\\n\\n\\n16.2.2. We may exercise this option by delivering a notice of intent to purchase to you within 30 days after the expiration or termination of this Agreement. During that 30-day period, you agree not to dispose of, transfer, or otherwise hinder our ability to exercise our rights with respect to your assets. 16.2.3. If we exercise our option to purchase, we may setoff all amounts due to us under this Agreement and the cost of the appraisal (if any), against any payment due to you. 16.2.4. If we do not exercise our rights to purchase your Delivery Vehicle(s), you must immediately make such modifications or alterations to the Delivery Vehicle(s) that may be needed to remove any Proprietary Marks and to otherwise distinguish the appearance of the vehicle(s) from those used by other Restaurants.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Without the prior written consent of the other party, neither party shall assign or transfer any of its rights or obligations hereunder, in whole or in part, to any third party, and any purported assignment without such prior written consent shall be null and void and of no force and effect; except that notice, but no consent shall be required for such assignment or transfer in connection with an internal reorganization or sale of the transferring party, including by merger or other business combination, or a sale of substantially all of the assets of the transferring party.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In the event of any termination of the Project by University, (a) University agrees to complete Phase I and II of the Project, and (b) ArTara will continue to provide annual funding until the completion of Phase II.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Company agrees that no other Distributor will be appointed in any other state as a Distributor unless it is either the Company or Distributor, save and except for the state of Florida.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              This Agreement shall automatically terminate in the event the Management Agreement is assigned or otherwise terminated.   \n",
       "3  You agree that, at our option, you will sell to us any or all your assets used to operate the Franchised Business (including equipment, fixtures, furnishings, Delivery Vehicles, supplies, and inventory) that we ask in writing to purchase. 16.2.1. The purchase price for such items will be equal to your depreciated cost (determined below) or fair market value, whichever is less. The cost will be determined based upon a five (5) year straight-line depreciation of original costs. For equipment that is five (5) or more years old, the parties agree that fair market value will be deemed to be ten percent (10%) of the equipment's original cost. The fair market value of tangible assets must be determined without reference to good will, going-concern value, or other intangible assets. Page 32 of 39\\n\\nSource: PF HOSPITALITY GROUP INC., 10-12G, 9/23/2015\\n\\n\\n\\n\\n\\n16.2.2. We may exercise this option by delivering a notice of intent to purchase to you within 30 days after the expiration or termination of this Agreement. During that 30-day period, you agree not to dispose of, transfer, or otherwise hinder our ability to exercise our rights with respect to your assets. 16.2.3. If we exercise our option to purchase, we may setoff all amounts due to us under this Agreement and the cost of the appraisal (if any), against any payment due to you. 16.2.4. If we do not exercise our rights to purchase your Delivery Vehicle(s), you must immediately make such modifications or alterations to the Delivery Vehicle(s) that may be needed to remove any Proprietary Marks and to otherwise distinguish the appearance of the vehicle(s) from those used by other Restaurants.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Without the prior written consent of the other party, neither party shall assign or transfer any of its rights or obligations hereunder, in whole or in part, to any third party, and any purported assignment without such prior written consent shall be null and void and of no force and effect; except that notice, but no consent shall be required for such assignment or transfer in connection with an internal reorganization or sale of the transferring party, including by merger or other business combination, or a sale of substantially all of the assets of the transferring party.   \n",
       "\n",
       "   labels  \n",
       "0       0  \n",
       "1       1  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Train and Test Separately\n",
    "To prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7026\n",
      "Validation samples: 1757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   1%|          | 48/7026 [00:47<1:53:17,  1.03 examples/s]"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the paraphrasing pipeline\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-small\", device=0)\n",
    "\n",
    "# Define a function for paraphrasing\n",
    "def paraphrase_batch(batch, num_return_sequences=3, num_beams=3):\n",
    "    \"\"\"\n",
    "    Paraphrase a batch of texts using the Hugging Face pipeline.\n",
    "    \"\"\"\n",
    "    inputs = [f\"paraphrase: {text}\" for text in batch['text']]\n",
    "    paraphrased_results = paraphraser(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        num_beams=num_beams,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    paraphrased_texts = []\n",
    "    # Process the results to extract 'generated_text'\n",
    "    if isinstance(paraphrased_results, list) and isinstance(paraphrased_results[0], list):\n",
    "        # Handle batched results: `paraphrased_results` is a nested list\n",
    "        for sublist in paraphrased_results:\n",
    "            paraphrased_texts.extend([result['generated_text'] for result in sublist])\n",
    "    elif isinstance(paraphrased_results, list):\n",
    "        # Handle unbatched results: `paraphrased_results` is a flat list\n",
    "        paraphrased_texts.extend([result['generated_text'] for result in paraphrased_results])\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected paraphrased_results structure.\")\n",
    "\n",
    "    # Repeat labels for each paraphrased result\n",
    "    labels = sum([[label] * num_return_sequences for label in batch['labels']], [])\n",
    "\n",
    "    return {'text': paraphrased_texts, 'labels': labels}\n",
    "\n",
    "# Convert your DataFrame to a Dataset\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_test = Dataset.from_pandas(test_df)\n",
    "# Apply the paraphrasing function in batches\n",
    "batch_size = 16  # Adjust based on your GPU capacity\n",
    "paraphrased_dataset_train = dataset_train.map(\n",
    "    lambda batch: paraphrase_batch(batch, num_return_sequences=3),\n",
    "    batched=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Apply the paraphrasing function in batches\n",
    "batch_size = 16  # Adjust based on your GPU capacity\n",
    "paraphrased_dataset_test = dataset_test.map(\n",
    "    lambda batch: paraphrase_batch(batch, num_return_sequences=3),\n",
    "    batched=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "df_augmented_train = pd.DataFrame({\n",
    "    'text': [item for sublist in paraphrased_dataset_train['text'] for item in sublist] if isinstance(paraphrased_dataset_train['text'][0], list) else paraphrased_dataset_train['text'],\n",
    "    'labels': [item for sublist in paraphrased_dataset_train['labels'] for item in sublist] if isinstance(paraphrased_dataset_train['labels'][0], list) else paraphrased_dataset_train['labels']\n",
    "})\n",
    "df_augmented_test = pd.DataFrame({\n",
    "    'text': [item for sublist in paraphrased_dataset_test['text'] for item in sublist] if isinstance(paraphrased_dataset_test['text'][0], list) else paraphrased_dataset_test['text'],\n",
    "    'labels': [item for sublist in paraphrased_dataset_test['labels'] for item in sublist] if isinstance(paraphrased_dataset_test['labels'][0], list) else paraphrased_dataset_test['labels']\n",
    "})\n",
    "\n",
    "\n",
    "# Append augmented data to the original DataFrame\n",
    "train_df = pd.concat([train_df, df_augmented_train], ignore_index=True)\n",
    "test_df = pd.concat([test_df, df_augmented_test], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6148\n",
      "Validation samples: 2635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 7026/7026 [00:00<00:00, 293520.65 examples/s]\n",
      "Filter: 100%|██████████| 1757/1757 [00:00<00:00, 220211.93 examples/s]\n",
      "Map: 100%|██████████| 7026/7026 [00:02<00:00, 2936.78 examples/s]\n",
      "Map: 100%|██████████| 1757/1757 [00:00<00:00, 2887.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "# Define a tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],        # The text column to tokenize\n",
    "        padding=\"max_length\",    # Pad all sequences to the same maximum length\n",
    "        truncation=True,         # Truncate sequences longer than the model's limit\n",
    "        max_length=128           # Maximum sequence length for BERT-based models\n",
    "    )\n",
    "\n",
    "\n",
    "# Convert dataframe to Hugging Face Dataset\n",
    "hf_dataset_train = Dataset.from_pandas(train_df)\n",
    "hf_dataset_test = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Drop rows with missing text values\n",
    "hf_dataset_train = hf_dataset_train.filter(lambda example: example[\"text\"] is not None)\n",
    "hf_dataset_test = hf_dataset_test.filter(lambda example: example[\"text\"] is not None)\n",
    "\n",
    "\n",
    "tokenized_train_dataset = hf_dataset_train.map(tokenize_function, batched=True).remove_columns([\"text\"])\n",
    "tokenized_test_dataset = hf_dataset_test.map(tokenize_function, batched=True).remove_columns([\"text\"])\n",
    "\n",
    "train_dataset = tokenized_train_dataset\n",
    "val_dataset = tokenized_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load ContractBERT for binary classification (2 labels: 0 and 1)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"nlpaueb/legal-bert-base-uncased\",\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "model = model.to(\"cuda\")  # Explicitly move the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\DS340\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,                     # Evaluate every 500 steps\n",
    "    save_steps=500,                     # Save every 500 steps\n",
    "    learning_rate=3e-5,                 # Adjusted learning rate\n",
    "    per_device_train_batch_size=16,     # Smaller batch size to fit GPU memory\n",
    "    gradient_accumulation_steps=2,      # Effective batch size = 32\n",
    "    per_device_eval_batch_size=16,      \n",
    "    num_train_epochs=10,                 # More epochs for better training\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=4,\n",
    "    fp16=True,                          # Mixed precision for better GPU utilization\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"error\",\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine annealing for better convergence\n",
    "    warmup_steps=500,                   # Gradual learning rate warmup\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=1.0,  # Limit gradient norm for stability\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\DS340\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,                     # Evaluate every 500 steps\n",
    "    save_steps=500,                     # Save every 500 steps\n",
    "    learning_rate=3e-5,                 # Adjusted learning rate\n",
    "    per_device_train_batch_size=16,     # Smaller batch size to fit GPU memory\n",
    "    gradient_accumulation_steps=2,      # Effective batch size = 32\n",
    "    per_device_eval_batch_size=16,      \n",
    "    num_train_epochs=10,                 # More epochs for better training\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=4,\n",
    "    fp16=True,                          # Mixed precision for better GPU utilization\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"error\",\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine annealing for better convergence\n",
    "    warmup_steps=500,                   # Gradual learning rate warmup\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=1.0,  # Limit gradient norm for stability\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                       # Pretrained model\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=train_dataset,       # Training dataset\n",
    "    eval_dataset=val_dataset           # Validation dataset\n",
    ")\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1100 [01:19<2:07:03,  7.00s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start fine-tuning the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\DS340\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\DS340\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start fine-tuning the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the best model to a specific directory\n",
    "save_directory = 'final_model_terms_and_conditions'\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the accuracy metric using the `evaluate` library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)  # Get predicted class\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"]}\n",
    "\n",
    "# Define TrainingArguments for evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final_model_terms_and_conditions\",          # Directory to save results\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate at the end of every epoch\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    logging_dir=\"./logs\",           # Directory for logs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model for evaluation\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"final_confidentiality_and_NDA\")\n",
    "\n",
    "# Create a Trainer object for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,  # Validation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f\"Validation Accuracy: {results['eval_accuracy']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
